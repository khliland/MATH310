{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATH310 - Lecture_notes2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">  \n",
    "\n",
    "__Vector spaces, basis, Gram-Schmidt and Least squares regression (see the [VMLS-slides, chapters 5-9](https://web.stanford.edu/~boyd/vmls/vmls-slides.pdf#page=99))__\n",
    "    \n",
    "    \n",
    "* The Gram-Schmidt orthogonalization process \n",
    "* QR-factorization                           \n",
    "* A least squares example                    \n",
    "* Solving Least squares problems by QR       \n",
    "* Linear Least Squares problems            \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The [Gram-Schmidt Process](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process)\n",
    "\n",
    "<font size=\"4\">\n",
    "If $W\\subseteq {\\bf R}^n$ is a subspace spanned by an orthogonal basis $\\{{\\bf v}_1,...,{\\bf v}_p\\}$, and ${\\bf y}\\in {\\bf R}^n$, then the orthogonal projection of ${\\bf y}$ onto $W$ is \n",
    "\n",
    "$$\\hat{\\bf y}=proj_W{\\bf y}=\\sum_{i=1}^{p}c_i{\\bf v}_i,$$ \n",
    "\n",
    "with $c_i=\\frac{{\\bf v}_i\\cdot{\\bf y}}{{\\bf v}_i\\cdot{\\bf v}_i}=\\frac{{\\bf v}_i^t{\\bf y}}{{\\bf v}_i^t{\\bf v}_i}$ for $i=1,...,p$, and ${\\bf z}={\\bf y}-\\hat{\\bf y}\\in W^{\\bot}$.\n",
    "    \n",
    "If we normalize the basis vectors, i.e. ${\\bf u}_1=\\frac{{\\bf v}_1}{\\|{\\bf v}_1\\|},...,{\\bf u}_p=\\frac{{\\bf v}_p}{\\|{\\bf v}_p\\|}$, the orthogonal projection of ${\\bf y}$ onto $W$ simplifies to\n",
    "    $$\\hat{\\bf y}=proj_W{\\bf y}=\\sum_{i=1}^{p}d_i{\\bf u}_i = \\sum_{i=1}^{p}{\\bf u}_i({\\bf u}_i^t{\\bf y}),$$ \n",
    "where $d_i = c_i\\|{\\bf v}_1\\|={\\bf u}_i^t{\\bf y}$.\n",
    "</font>     \n",
    "\n",
    "### In the following we will apply the principles of orthogonal projections to construct an _orthogonal_ (or even orthonormal if desired) basis $V=\\{{\\bf v}_1,...,{\\bf v}_n\\}$ from _any_ given basis $B=\\{{\\bf x}_1,...,{\\bf x}_n\\}$ for a subspace $W\\subseteq{\\bf R}^n$\n",
    " \n",
    "  \n",
    "  \n",
    "  \n",
    "## Theorem ([Gram-Schmidt process](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process))\n",
    "<font size=\"4\">\n",
    "Let $\\{{\\bf x}_1,...,{\\bf x}_p\\}$ be some basis for a subspace $W\\subseteq{\\bf R}^n$. Define \n",
    "\n",
    "$${\\bf v}_1={\\bf x}_1 \\textrm{ and } {\\bf u}_1 = \\frac{{\\bf v}_1}{\\|{\\bf v}_1\\|}$$ \n",
    "\n",
    "$${\\bf v}_2={\\bf x}_2-\\frac{{\\bf x}_2\\cdot{\\bf v}_1}{{\\bf v}_1\\cdot{\\bf v}_1}{\\bf v}_1 = {\\bf x}_2-{\\bf u}_1({\\bf u}_1^t{\\bf x}_2)\\textrm{ and } {\\bf u}_2 = \\frac{{\\bf v}_2}{\\|{\\bf v}_2\\|}$$ \n",
    "\n",
    "$$\\vdots$$\n",
    "\n",
    "$${\\bf v}_p={\\bf x}_p-\\frac{{\\bf x}_p\\cdot{\\bf v}_1}{{\\bf v}_1\\cdot{\\bf v}_1}{\\bf v}_1-\\frac{{\\bf x}_p\\cdot{\\bf v}_2}{{\\bf\n",
    "v}_2\\cdot{\\bf v}_2}{\\bf v}_2-\\ldots -\\frac{{\\bf x}_p\\cdot{\\bf v}_{p-1}}{{\\bf v}_{p-1}\\cdot{\\bf v}_{p-1}}{\\bf v}_{p-1} $$\n",
    "$$= {\\bf v}_p - {\\bf u}_1({\\bf u}_1^t{\\bf x}_p) - {\\bf u}_2({\\bf u}_2^t{\\bf x}_p) -\\ldots - {\\bf u}_{p-1}({\\bf u}_{p-1}^t{\\bf x}_p) \\textrm{ and } {\\bf u}_p = \\frac{{\\bf v}_p}{\\|{\\bf v}_p\\|}.$$\n",
    "\n",
    "   \n",
    "Then $V=\\{{\\bf v}_1,...,{\\bf v}_p\\}$ is an orthogonal basis for $W$, $U=\\{{\\bf u}_1,...,{\\bf u}_p\\}$ is an orthonormal basis for $W$ and\n",
    "\n",
    "$$\\textsf{Span}\\{{\\bf v}_1,...,{\\bf v}_k\\}=\\textsf{Span}\\{{\\bf u}_1,...,{\\bf u}_k\\}=\\textsf{Span}\\{{\\bf x}_1,...,{\\bf x}_k\\}$$ \n",
    "\n",
    "for $1\\leq k\\leq p$.\n",
    "  \n",
    "  \n",
    "  \n",
    "<ins>Proof:</ins>  \n",
    "For $1\\leq k\\leq p$ we define $W_k=\\textsf{Span}\\{{\\bf x}_1,...,{\\bf x}_k\\}$. With ${\\bf v}_1 = {\\bf x}_1$ it holds that $W_1=\\textsf{Span}\\{{\\bf x}_1\\}=\\textsf{Span}\\{{\\bf v}_1\\}$. Assume further that we have constructed ${\\bf v}_1,...,{\\bf v}_k$\n",
    "such that $\\{{\\bf v}_1,...,{\\bf v}_k\\}$ forms an orthogonal basis\n",
    "for $W_k$. Define \n",
    "\n",
    "$${\\bf v}_{k+1}={\\bf x}_{k+1}-proj_{W_k}({\\bf x}_{k+1})$$\n",
    "\n",
    "By __the orthogonal decomposition theorem__, ${\\bf v}_{k+1}\\in W_k^{\\bot}$. By definition, the projection $proj_{W_k}({\\bf x}_{k+1})\\in W_k\\subset W_{k+1}$.  Because ${\\bf x}_{k+1}\\in W_{k+1}$, it follows that ${\\bf v}_{k+1}\\in W_{k+1}$, and because ${\\bf v}_{k+1}$ is not an element in $W_k$ then \\ ${\\bf v}_{k+1}\\neq {\\bf 0}$. With this $\\{{\\bf v}_1,...,{\\bf v}_{k+1}\\}$ is an orthogonal set contained in $W_{k+1}$ and also an orthogonal basis for $W_{k+1}$ since none of the these vectors are the null vector.  \n",
    "    \n",
    "This argument can be repeated until $k+1=p$. The fact that $W =\\textsf{Span}\\{{\\bf u}_1,...,{\\bf u}_p\\}$ follows directly from the normalizations, i.e. $\\|{\\bf v}_k\\|{\\bf u}_k={\\bf v}_k$ for $k = 1,...,p$.\n",
    "</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [QR - factorization](https://en.wikipedia.org/wiki/QR_decomposition)\n",
    "<font size=\"4\">\n",
    "\n",
    "When applied to the columns a matrix with linearly independent columns, the Gram-Schmidt process results in an important factorization technique that is useful for practical applications.  The particular matrix factorization is known as {\\em QR-decomposition}.\n",
    "  \n",
    "  \n",
    "__Theorem (QR - decomposition)__\n",
    "Let $A=[{\\bf x}_1,...,{\\bf x}_n]$ be an $m\\times n$-matrix of rank $n$ (the $n$ columns in $A$ are linearly independent). Then\n",
    " $A$ can be factorized as a matrix product $A=QR$, where $Q$ is an $m\\times n$-matrix with orthonormal columns, and $R$ is an invertible and upper triangular $n\\times n$-matrix with positive entries on the diagonal.\n",
    "  \n",
    "  \n",
    "  \n",
    "<ins>Proof:</ins>  \n",
    "The columns in $A$ form a basis $\\{{\\bf x}_1,...,{\\bf x}_n\\}$ for $\\textsf{Col}(A)$, and through GS-theorem we can construct\n",
    "an orthogonal basis $\\{{\\bf v}_1,...,{\\bf v}_n\\}$ that can be modified to an orthonormal basis $\\{{\\bf u}_1,...,{\\bf u}_n\\}$\n",
    "for $\\textsf{Col}(A)$ by letting ${\\bf u}_i$ be the normalized version of each basis vector ${\\bf v}_i$. Define\n",
    "\n",
    "$$Q = [{\\bf u}_1\\ {\\bf u}_2 \\cdots {\\bf u}_n].$$\n",
    "\n",
    "For k=1,...,n, then ${\\bf x}_k\\in \\textsf{Span}\\{{\\bf x}_1,...,{\\bf x}_k\\}=\\textsf{Span}\\{{\\bf u}_1,...,{\\bf u}_k\\}$, then we can find  coefficients such that \n",
    "\n",
    "$${\\bf x}_k=r_{1k}{\\bf u}_1+\\cdots+r_{kk}{\\bf u}_k+0{\\bf u}_{k+1}+\\cdots+0{\\bf u}_n.$$\n",
    "\n",
    "Note that $r_{kk}\\neq 0$ as ${\\bf v}_k$ by construction is not orthogonal with ${\\bf x}_k$, and with this,\n",
    " ${\\bf u}_k$ is not orthogonal with  ${\\bf x}_k$. We can assume that $r_{kk}>0$ (if not, replace ${\\bf u}_k$ with $-{\\bf u}_k$ that also is a unit vector, and change the sign of $r_{kk}$). Alternatively we can write  ${\\bf x}_k=Q{\\bf r}_k$,  where\n",
    " \n",
    " $${\\bf r}_k=\\left[\n",
    "\\begin{array}{c}\n",
    "  r_{1k} \\\\\n",
    "  \\vdots \\\\\n",
    "  r_{kk} \\\\\n",
    "  0 \\\\\n",
    "  \\vdots \\\\\n",
    "  0 \\\\\n",
    "\\end{array}\n",
    "\\right] = \\left[\n",
    "\\begin{array}{c}\n",
    "  {\\bf u}_1\\cdot {\\bf x}_k\\\\\n",
    "  \\vdots \\\\\n",
    "  {\\bf u}_k\\cdot {\\bf x}_k\\\\\n",
    "  0 \\\\\n",
    "  \\vdots \\\\\n",
    "  0 \\\\\n",
    "\\end{array}\n",
    "\\right].$$\n",
    "\n",
    "If we set $R=[{\\bf r}_1\\ \\cdots\\ {\\bf r}_n]$ then \n",
    "\n",
    "$$A=[{\\bf x}_1 \\ \\cdots\\ {\\bf x}_n]=[Q{\\bf r}_1\\ \\cdots\\ Q{\\bf r}_n]= QR.$$ \n",
    "\n",
    "Through construction of each column ${\\bf r}_k$ in $R$ it follows that $R$ is upper triangular.\n",
    "  \n",
    "The following method to find the $QR$-factorization of a $m\\times n$ - matrix $A$ with linearly independent columns is a consequence of theorem 12:  \n",
    "\n",
    "\n",
    "* Use the Gram - Schmidt process to find an orthogonal basis $\\{{\\bf v}_1,...,{\\bf v}_n\\}$ for the column space of $A$.\n",
    "* Normalize the basis found above to create the orthonormal basis $\\{{\\bf u}_1,...,{\\bf u}_n\\}$. \n",
    "* Set $Q =[{\\bf u}_1\\ {\\bf u}_2\\ \\cdots {\\bf u}_n]$ and $R = [{\\bf r}_1\\ {\\bf r}_2\\ \\cdots {\\bf r}_n]$. \n",
    "* From the orthonormal columns of $Q$ ($Q^tQ=I_n$) we have \n",
    "    \n",
    "    $$Q^tA = Q^t(QR)=(Q^tQ)R = R.$$ \n",
    "    \n",
    "</font>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1:\n",
    "\n",
    "<font size=\"4\">\n",
    "Assume that $A$ has linearly independent columns, and that $A=QR$ is a QR-factorization of $A$. Recall that the projection mapping onto $Col(A)$ is defined as $P = A(A^tA)^{-1}A^t$. \n",
    "\n",
    "Show the identity $A(A^tA)^{-1}A^t = QQ^t$ to conclude that the projection mapping $P = QQ^t$.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Linear Least Squares problems  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "    \n",
    "__Example__\n",
    "\n",
    "The System $A{\\bf x}={\\bf b}$ where\n",
    "$$A=\\left[%\n",
    "\\begin{array}{cc}\n",
    "  2 & 0 \\\\\n",
    "  0 & 1 \\\\\n",
    "  2 & 2 \\\\\n",
    "\\end{array}%\n",
    "\\right] \\textsf{ and }{\\bf b}=\\left[%\n",
    "\\begin{array}{c}\n",
    "  1 \\\\\n",
    "  2 \\\\\n",
    "  3 \\\\\n",
    "\\end{array}%\n",
    "\\right]$$ is inconsistent, and we want to find a least squares solution $\\hat{\\bf x}$ instead.\n",
    "    \n",
    "\n",
    "First calculate $A^tA$ and $A^t{\\bf b}$, to obtain\n",
    "the corresponding system of normal equations $A^tA\\hat{\\bf\n",
    "x}=A^t{\\bf b}$. \n",
    "       \n",
    "$$A^tA=\\left[%\n",
    "\\begin{array}{ccc}\n",
    "  2 & 0 & 2\\\\\n",
    "  0 & 1 & 2\\\\\n",
    "\\end{array}%\n",
    "\\right]\\left[%\n",
    "\\begin{array}{cc}\n",
    "  2 & 0 \\\\\n",
    "  0 & 1 \\\\\n",
    "  2 & 2 \\\\\n",
    "\\end{array}%\n",
    "\\right]=\\left[%\n",
    "\\begin{array}{cc}\n",
    "  8 & 4 \\\\\n",
    "  4 & 5 \\\\\n",
    "\\end{array}%\n",
    "\\right]$$\n",
    "$$A^t{\\bf b}=\\left[%\n",
    "\\begin{array}{ccc}\n",
    "  2 & 0 & 2\\\\\n",
    "  0 & 1 & 2\\\\\n",
    "\\end{array}%\n",
    "\\right]\\left[%\n",
    "\\begin{array}{c}\n",
    "  1 \\\\\n",
    "  2 \\\\\n",
    "  3 \\\\\n",
    "\\end{array}%\n",
    "\\right]=\\left[%\n",
    "\\begin{array}{c}\n",
    "  8 \\\\\n",
    "  8 \\\\\n",
    "\\end{array}%\n",
    "\\right].$$ \n",
    "    \n",
    "The normal equations are always consistent, and the solution(s) can be obtained by solving\n",
    "$$\\left[%\n",
    "\\begin{array}{cc}\n",
    "  8 & 4 \\\\\n",
    "  4 & 5 \\\\\n",
    "\\end{array}%\n",
    "\\right]\\left[%\n",
    "\\begin{array}{c}\n",
    "  x_1 \\\\\n",
    "  x_2 \\\\\n",
    "\\end{array}%\n",
    "\\right]=\\left[%\n",
    "\\begin{array}{c}\n",
    "  8 \\\\\n",
    "  8 \\\\\n",
    "\\end{array}%\n",
    "\\right]$$ Through row reduction of the extended matrix\n",
    "$$\\left[%\n",
    "\\begin{array}{ccc}\n",
    "  8 & 4 & 8\\\\\n",
    "  4 & 5 & 8\\\\\n",
    "\\end{array}%\n",
    "\\right]\\sim \\left[%\n",
    "\\begin{array}{ccc}\n",
    "  1 & 0 & \\frac{1}{3}\\\\\n",
    "  0 & 1 & \\frac{4}{3}\\\\\n",
    "\\end{array}%\n",
    "\\right]\\Rightarrow \\hat{\\bf x}=\\left[%\n",
    "\\begin{array}{c}\n",
    "  \\frac{1}{3}\\\\\n",
    "  \\frac{4}{3} \\\\\n",
    "\\end{array}%\n",
    "\\right].$$\n",
    "Finally, the least squares residual vector\n",
    "$${\\bf z}={\\bf b}-A\\hat{\\bf x}=\\left[%\n",
    "\\begin{array}{c}\n",
    "  1 \\\\\n",
    "  2 \\\\\n",
    "  3 \\\\\n",
    "\\end{array}%\n",
    "\\right]-\\left[%\n",
    "\\begin{array}{cc}\n",
    "  2 & 0 \\\\\n",
    "  0 & 1 \\\\\n",
    "  2 & 2 \\\\\n",
    "\\end{array}%\n",
    "\\right]\\left[%\n",
    "\\begin{array}{c}\n",
    "  \\frac{1}{3}\\\\\n",
    "  \\frac{4}{3} \\\\\n",
    "\\end{array}%\n",
    "\\right]=\\left[%\n",
    "\\begin{array}{c}\n",
    "  1 \\\\\n",
    "  2 \\\\\n",
    "  3 \\\\\n",
    "\\end{array}%\n",
    "\\right]-\\left[%\n",
    "\\begin{array}{c}\n",
    "  \\frac{2}{3} \\\\\n",
    "  \\frac{4}{3}\\\\\n",
    "  \\frac{10}{3} \\\\\n",
    "\\end{array}%\n",
    "\\right] =\\left[%\n",
    "\\begin{array}{r}\n",
    " \\frac{1}{3} \\\\\n",
    "  \\frac{2}{3} \\\\\n",
    "  -\\frac{1}{3} \\\\\n",
    "\\end{array}%\n",
    "\\right],$$ and the least squares error\n",
    "$$\\|{\\bf z}\\|=\\|{\\bf b}-A\\hat{\\bf x}\\|=\\sqrt{\\frac{1}{9}+\\frac{4}{9}+\\frac{1}{9}}=\\sqrt{\\frac{2}{3}}.$$\n",
    "    \n",
    "__Exercise:__ \n",
    "    \n",
    "* Verify that ${\\bf z}$ is orthogonal to the columns of $A$ by calculating ${\\bf z}^tA$.  \n",
    "* Calculate the angle between ${\\bf b}$ and ${\\bf z}$.\n",
    "* Calculate the angle between ${\\bf b}$ and the subspace spanned by the columns of $A$.    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Least Squares problems by QR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "\n",
    "We can apply the QR-factorization of $A$ (assuming $A$ has linearly independent columns) to calculate the solution vector $\\hat{\\bf x}$ resulting in the projection $\\hat{\\bf b}=proj_A({\\bf b})\\in Col(A)$:\n",
    "\n",
    "![Loss](Figur2.png)\n",
    "\n",
    "Recall that the least squares solution of the system \n",
    "    \n",
    "$$A{\\bf x}={\\bf b}$$\n",
    "\n",
    "coincides with the (non-empty) set of solutions of the _normal\n",
    "equations_ \n",
    "    \n",
    "$$A^tA\\hat{\\bf x}=A^t{\\bf b},$$ \n",
    "    \n",
    "and that the solution $\\hat{\\bf x}=(A^tA)^{-1}A^t{\\bf b}$ is unique when $A$ has linearly independent columns.\n",
    "\n",
    "We can take advantage of the QR factorization as a practical tool to compute a least squares solution:   \n",
    "    \n",
    "    \n",
    "__Theorem (QR-solution of least squares problems)__ \n",
    "    \n",
    "Assume $A$ is a $m\\times n$ - matrix with linearly independent columns and QR - factorization $A=QR$. Then for every ${\\bf b}\\in{\\bf R}^m$ we can find a unique least squares solution $\\hat{\\bf x}$ of the system $A{\\bf x}={\\bf b}$ by solving the equivalent system \n",
    "    \n",
    "$$R{\\bf x}=Q^t{\\bf b},$$ and this solution is given by\n",
    "\n",
    "$$\\hat{\\bf x}=R^{-1}Q^t{\\bf b}.$$\n",
    "\n",
    "__Proof:__\n",
    "    \n",
    "By the QR-factorization of $A=QR$ where $R$ is upper triangular and invertible, the systems $A{\\bf x}={\\bf b}$ and $QR{\\bf x}={\\bf b}$ are equivalent. By defining ${\\bf y}=R{\\bf x}$, an equivalent formulation of the above system is \n",
    "    \n",
    "$$Q{\\bf y}={\\bf b}.$$\n",
    "    \n",
    "The normal equations of the last system are particularly simple, i.e. \n",
    "\n",
    "$$(Q^tQ{\\bf y}=){\\bf y}=Q^t{\\bf b},$$ \n",
    "    \n",
    "implying that ${\\bf y}=R{\\bf x}=Q^t{\\bf b}$ and finally \n",
    "    \n",
    "$$\\hat{\\bf x}=R^{-1}Q^t{\\bf b}$$ \n",
    "    \n",
    "since the upper triangular $R$ is invertible when $A$ has linearly independent columns.\n",
    "\n",
    "__Note:__ An explicit inversion of the upper triangular matrix $R$ is actually not required. To find the least squares solution $\\hat{\\bf x}$ we can solve the system $R{\\bf x}=Q^t{\\bf b}$ much faster by the familiar [__back-substitution__](https://en.wikipedia.org/wiki/Triangular_matrix#Forward_and_back_substitution) method. \n",
    "\n",
    "You can read more about the Gram-Schmidt algorithm in the [VMLS-notes](http://stanford.edu/class/ee103/lectures/linear-indep_slides.pdf#page=15), and in [VLSM, section 5.4](https://web.stanford.edu/~boyd/vmls/vmls.pdf#page=107).\n",
    " </font>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least squares Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "\n",
    "In statistical data analysis, linear least squares problems frequently come up known as _Linear Regression_. In statistical applications, we often \n",
    "use a notation that differs from the one we used in section (6.5). \n",
    "Instead of the formulation $A{\\bf x}={\\bf b}$  statisticians prefer the notation \n",
    "    \n",
    "$${\\bf y}=X{\\bf \\beta},$$\n",
    "    \n",
    "where it is usual to refer to $X$ as the _data matrix_ (or design matrix) of measured _explanatory variables_, \n",
    "${\\bf \\beta}$ is the _parameter vector_ with _regression coefficients_ and ${\\bf y}$ \n",
    "as the corresponding vector of _response values_.\n",
    "\n",
    "In the simplest case of a linear regression we fit a straight line to a set of points in the plane. The formula for a such a line is\n",
    "    \n",
    "$$y=\\beta_0+\\beta_1x.$$\n",
    "    \n",
    "By fixing the values of $\\beta_0$ and $\\beta_1$ we obtain a line\n",
    "as in the following figure:\n",
    "\n",
    "![Loss](Figur3.png)\n",
    "\n",
    "The challenge here is to determine values for the regression coefficients\n",
    "  $\\beta_0$ and $\\beta_1$ so that the line fits as close as possible to a set of points\n",
    " $(x_1, y_1),\n",
    "(x_2,y_2),...,(x_n, y_n)$.\n",
    "\n",
    "Specifically, we want the differences (often called\n",
    "residuals) between the observed response values $y_k$\n",
    "and the {\\em predicted} response values $\\hat{y}_k=\\beta_0+\\beta_1x_k$\n",
    "to be as small as possible.\n",
    "\n",
    "The problem can be represented as a linear system with the following matrix-vector equation:\n",
    "\n",
    "$$X{\\bf \\beta}={\\bf y}\\textsf{ where } X=\\left[%\n",
    "\\begin{array}{cc}\n",
    "  1 & x_1 \\\\\n",
    "  1 & x_2 \\\\\n",
    "  \\vdots & \\vdots \\\\\n",
    "  1 & x_n \\\\\n",
    "\\end{array}%\n",
    "\\right],\\ {\\bf \\beta}=\\left[%\n",
    "\\begin{array}{c}\n",
    "  \\beta_0 \\\\\n",
    "  \\beta_1 \\\\\n",
    "\\end{array}%\n",
    "\\right] \\textsf{ and }{\\bf y}=\\left[%\n",
    "\\begin{array}{c}\n",
    "  y_1 \\\\\n",
    "  y_2 \\\\\n",
    "   \\vdots \\\\\n",
    "  y_n \\\\\n",
    "\\end{array}%\n",
    "\\right].$$ \n",
    "    \n",
    "Usually, the system does not have an exact solution so \n",
    "we will look for the least squares solution. The normal equations in this case become  \n",
    "\n",
    "$$X^tX{\\bf \\beta}=X^t{\\bf y},$$ \n",
    "    \n",
    "and if $X^tX$ is invertible, then the _estimated_ least squares regression coefficients (the least squares solution) \n",
    "    \n",
    "$$\\hat{\\bf \\beta}=(X^tX)^{-1}X^t{\\bf y} \\textsf{ and estimated response } \\hat{\\bf y}=X\\hat{\\bf \\beta}.$$ \n",
    "\n",
    "</font>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "\n",
    "Calculate the regression coefficients in an least squares\n",
    "regression model to fit a line to the points.  \n",
    "\n",
    "$$(1,0),\\ (2,1),\\ (4,2),\\ (5,3).$$ \n",
    "\n",
    "From these points we can define\n",
    "\n",
    "$$X=\\left[%\n",
    "\\begin{array}{cc}\n",
    "  1 & 1 \\\\\n",
    "  1 & 2 \\\\\n",
    "  1 & 4 \\\\\n",
    "  1 & 5 \\\\\n",
    "\\end{array}%\n",
    "\\right]\\textsf{ and }{\\bf y}=\\left[%\n",
    "\\begin{array}{c}\n",
    "  0 \\\\\n",
    "  1  \\\\\n",
    "  2 \\\\\n",
    "  3 \\\\\n",
    "\\end{array}%\n",
    "\\right].$$ \n",
    "    \n",
    "$$X^tX=\\left[%\n",
    "\\begin{array}{cccc}\n",
    "  1 & 1 & 1 & 1\\\\\n",
    "  1 & 2 & 4 & 5\\\\\n",
    "\\end{array}%\n",
    "\\right]\\left[%\n",
    "\\begin{array}{cc}\n",
    "  1 & 1 \\\\\n",
    "  1 & 2 \\\\\n",
    "  1 & 4 \\\\\n",
    "  1 & 5 \\\\\n",
    "\\end{array}%\n",
    "\\right]=\\left[%\n",
    "\\begin{array}{cc}\n",
    "  4 & 12 \\\\\n",
    "  12 & 46 \\\\\n",
    "\\end{array}%\n",
    "\\right]$$\n",
    "    \n",
    "and \n",
    "    \n",
    "$$X^t{\\bf y}=\\left[%\n",
    "\\begin{array}{c}\n",
    "  6 \\\\\n",
    "  25  \\\\\n",
    "\\end{array}%\n",
    "\\right].$$ \n",
    "The normal equations are\n",
    "$$\\left[%\n",
    "\\begin{array}{cc}\n",
    "  4 & 12 \\\\\n",
    "  12 & 46 \\\\\n",
    "\\end{array}%\n",
    "\\right]\\left[%\n",
    "\\begin{array}{c}\n",
    "  \\beta_0 \\\\\n",
    "  \\beta_1  \\\\\n",
    "\\end{array}%\n",
    "\\right]=\\left[%\n",
    "\\begin{array}{c}\n",
    "  6 \\\\\n",
    "  25  \\\\\n",
    "\\end{array}%\n",
    "\\right].$$ \n",
    "\n",
    "The system can be solved either by calculating \n",
    "$(X^tX)^{-1}$ or by Gaussian elimination applied to the extended matrix in echelon form:\n",
    "\n",
    "$$U=\\left[%\n",
    "\\begin{array}{ccc}\n",
    "  4 & 12 & 6\\\\\n",
    "  12 & 46 & 25\\\\\n",
    "\\end{array}%\n",
    "\\right]\\sim \\left[%\n",
    "\\begin{array}{ccc}\n",
    "  4 & 12 & 6\\\\\n",
    "  0 & 10 & 7\\\\\n",
    "\\end{array}%\n",
    "\\right] \\sim \\left[%\n",
    "\\begin{array}{ccc}\n",
    "  1 & 3 & 1.5\\\\\n",
    "  0 & 1 & 0.7\\\\\n",
    "\\end{array}%\n",
    "\\right]\\sim \\left[%\n",
    "\\begin{array}{ccr}\n",
    "  1 & 0 & -0.6\\\\\n",
    "  0 & 1 & 0.7\\\\\n",
    "\\end{array}%\n",
    "\\right],$$ \n",
    "    \n",
    "then  \n",
    "    \n",
    "$$\\hat{\\bf \\beta}=\\left[%\n",
    "\\begin{array}{r}\n",
    "  -0.6 \\\\\n",
    "  0.7  \\\\\n",
    "\\end{array}%\n",
    "\\right].$$\n",
    "\n",
    "We can visualize the resulting fitted line:\n",
    "\n",
    "![Loss](Figur4.png)\n",
    "\n",
    "__Exercise 2:__ \n",
    "    \n",
    "Use a QR-factorization of $X$ to calculate the regression coefficients $\\hat{\\bf \\beta}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "\n",
    "It is straight forward to extend the general method also to fit models (finding the regression coefficients $\\beta_0,\\beta_1,...,\\beta_p$)\n",
    "for situations we $p$ explanatory variables are measured for each observation. With the data points given as\n",
    "\n",
    "$$(x_{11},x_{12},...,x_{1p},y_1)$$\n",
    "$$(x_{21},x_{22},...,x_{2p},y_2)$$\n",
    "$$\\vdots$$\n",
    "$$(x_{n1},x_{n2},...,x_{np},y_n)$$\n",
    "\n",
    "we seek the estimation of regression coefficients for the model\n",
    "\n",
    "$$y = \\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_px_p$$\n",
    "\n",
    "so that the model fits best in the least squares sense.\n",
    "\n",
    "This can be set up as the matrix-vector equation $$X{\\bf \\beta}+{\\bf \\epsilon} = {\\bf y},$$\n",
    "where ${\\bf \\epsilon = y} -X{\\bf \\beta}$ is the _residual vector_ that describes\n",
    "the residual of each observation (data point) from the fit. (Any equation of this form is referred to as a _linear model_.)  \n",
    "\n",
    "$$\\textsf{Here } X=\\left[%\n",
    "\\begin{array}{ccccc}\n",
    "  1 & x_{11} & x_{12}& ... & x_{1p} \\\\\n",
    "  1 & x_{21} & x_{22}& ... & x_{2p} \\\\\n",
    "  \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "   1 & x_{n1} & x_{n2}& ... & x_{np} \\\\\n",
    "\\end{array}%\n",
    "\\right],$$  \n",
    "    \n",
    " $${\\bf \\beta}=\\left[%\n",
    "\\begin{array}{c}\n",
    "  \\beta_0 \\\\\n",
    "  \\beta_1 \\\\\n",
    "  \\vdots  \\\\\n",
    "  \\beta_p \\\\\n",
    "\\end{array}%\n",
    "\\right] \\textsf{, }{\\bf y}=\\left[%\n",
    "\\begin{array}{c}\n",
    "  y_1 \\\\\n",
    "  y_2 \\\\\n",
    "   \\vdots \\\\\n",
    "  y_n \\\\\n",
    "\\end{array}%\n",
    "\\right]\\textsf{ and }{\\bf \\epsilon}=\\left[%\n",
    "\\begin{array}{c}\n",
    "  \\epsilon_1 \\\\\n",
    "  \\epsilon_2 \\\\\n",
    "   \\vdots \\\\\n",
    "  \\epsilon_n \\\\\n",
    "\\end{array}%\n",
    "\\right].$$  \n",
    "    \n",
    "We seek the minimization of the residual vector ${\\bf \\epsilon}$, which amounts\n",
    "to finding the the least squares solution of $X{\\bf \\beta}={\\bf y}$. \n",
    "The corresponding normal equations are  \n",
    "\n",
    "$$X^tX{\\bf \\beta}=X^t{\\bf y},$$\n",
    "    \n",
    "and if $X^tX$ is invertible, we can find the unique solution of estimated regression coefficients as\n",
    "\n",
    "$$\\hat{\\bf \\beta}=(X^tX)^{-1}X^t{\\bf y}.$$\n",
    "    \n",
    "</font>    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
