{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATH310 - Lecture_notes4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo-inverse\n",
    "\n",
    "<font size=\"4\">\n",
    "'\n",
    " \n",
    "\n",
    "__Definition ([the pseudoinverse of a matrix $A$ with full rank](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse#Linearly_independent_columns))__\n",
    "\n",
    "The _pseudo-inverse_ of a $m\\times n$ matrix $A$ with full rank $r=n$ ($n$ is the number of columns in $A$) is denoted $A^+$ and defined as\n",
    "    \n",
    "$$A^+=(A^tA)^{-1}A^t.$$    \n",
    "    \n",
    "Note that $A^+$ is $n\\times m$ and has the property $A^+A=I_n.$\n",
    "    \n",
    "__`Exercise 1:`__    \n",
    "Use the \"thin\" SVD of $A = U\\Sigma V^t$ to show that\n",
    "    \n",
    "$$A^+=V\\Sigma^{-1}U^t.$$\n",
    "    \n",
    "__`Exercise 2:`__  \n",
    "Show that the least squares solution of $A{\\bf x}={\\bf b}$\n",
    "is\n",
    "    \n",
    "$$\\hat{\\bf x}=A^+{\\bf b}$$\n",
    "    \n",
    "when $A$ has full rank. \n",
    "    \n",
    "</font>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The minimum norm solution of underdetermined systems\n",
    "\n",
    "<font size=\"4\">\n",
    "'\n",
    "    \n",
    "If $rank(A)=r<n$ we say that $A$ is _rank-deficient_.\n",
    "    \n",
    "We say that linear systems $A{\\bf x} = {\\bf b}$ with rank-deficient coefficient matrices are _underdetermined_. \n",
    "    \n",
    "'    \n",
    "    \n",
    "__Definition (the pseudoinverse of any matrix $𝐴$)__\n",
    "\n",
    "Lets extend the definition of the pseudo-inverse to also include matrices $A=U_r\\Sigma_r V_r^t=\\sum_{i=1}^{r}\\sigma_i{\\bf v}_i{\\bf u}_i^t$ of any rank $r$ based on the result of exercise 1:\n",
    "    \n",
    "$$A^+=V_r\\Sigma_r^{-1}U_r^t=\\sum_{i=1}^{r}\\sigma_i^{-1}{\\bf v}_i{\\bf u}_i^t.$$\n",
    "    \n",
    "    \n",
    "Underdetermined systems can still be solved, but there is no longer a unique solution $\\hat{\\bf x}$. \n",
    "    \n",
    "However\n",
    "    \n",
    "__Theorem (Minimum norm solution)__\n",
    "    \n",
    "If $rank(A)=r<n$, then there are infinitely many least squares solutions of $A{\\bf x}={\\bf b}$. Among all the least squares solutions, the particular solution\n",
    "    \n",
    "$${\\bf x}_0=V_r\\Sigma_r^{-1}U_r^t{\\bf b} = A^+{\\bf b}$$\n",
    "    \n",
    "has the smallest possible norm.  \n",
    "    \n",
    "__Proof:__   \n",
    "It is clear that ${\\bf x}_0\\in span(V_r) = Col(A^t)$, i.e. the solution ${\\bf x}_0$ is a linear combination of the right singular vectors ${\\bf V}_r = A^tU_r\\Sigma_r^{-1}$ that are all linear combinations of the rows in $A$. The assumption $r < n$ implies that the null-space $Nul(A)\\neq \\{\\bf 0 \\}$ ($\\dim(Nul(A)=n-r$). \n",
    "    \n",
    "Let $\\hat{\\bf x}\\neq {\\bf x}_0$ be any least squares solution, i.e.\n",
    "$A\\hat{\\bf x}=\\hat{\\bf b}$, where $\\hat{\\bf b}$ is the projection of ${\\bf b}$ onto $Col(A)$.\n",
    "Then $A\\hat{\\bf x}=\\hat{\\bf b}=A{\\bf x}_0$ and therefore $A(\\hat{\\bf x}-{\\bf x}_0)=\\hat{\\bf b}-\\hat{\\bf b}={\\bf 0}$, i.e. $\\hat{\\bf x}-{\\bf x}_0\\in Nul(A)$. The latter means that $(\\hat{\\bf x}-{\\bf x}_0)$ is orthogonal both to the rows in $A$, as well as every linear combination of these rows, such as ${\\bf x}_0$.\n",
    "    \n",
    "Therefore, by Pythagoras theorem we have\n",
    "\n",
    "$$\\|\\hat{\\bf x}\\|^2=\\|(\\hat{\\bf x}-{\\bf x}_0)+{\\bf x}_0\\|^2=\\|\\hat{\\bf x}-{\\bf x}_0\\|^2+\\|{\\bf x}_0\\|^2 > \\|{\\bf x}_0\\|^2.$$\n",
    "\n",
    "Finally we note that for any vector ${\\bf n}\\in Nul(A)$, $\\hat{\\bf x}={\\bf x}_0+{\\bf n}$ is a least squares solution because $A\\hat{\\bf x}=A({\\bf x}_0+{\\bf n})=A{\\bf x}_0+A{\\bf n}=\\hat{\\bf b}+{\\bf 0}=\\hat{\\bf b}$, and because $Nul(A)\\neq \\{\\bf 0 \\}$ there are infinitely many choices for ${\\bf n}.\\ \\blacksquare$\n",
    "\n",
    "__`Exercise 3:`__    \n",
    "Verify that ${\\bf x}_0=V_r\\Sigma_r^{-1}U_r^t{\\bf b}$ above really is a least sqares solution of $A{\\bf x}={\\bf b}$ when $rank(A)=r<n$.\n",
    "    \n",
    "</font>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [The condition number of a matrix](https://en.wikipedia.org/wiki/Condition_number#Matrices) and poorly conditioned systems\n",
    "\n",
    "'\n",
    "\n",
    "<font size=\"4\">\n",
    "\n",
    "If $A=\\sum_{i=1}^{r}s_i{\\bf u}_i{\\bf v}_i^t=U_r\\Sigma_rV_r^t$ has rank $r$, the associated matrix (operator) norm $\\|A\\|_2$ defined as the supremum of\n",
    "${\\|A{\\bf x}\\|_2}$ over all unit vectors ${\\bf x}\\in \\mathbb{S}^{n-1} =\\{{\\bf x}|\\ {\\bf x}^t{\\bf x}=1\\} \\subseteq \\mathbb{R}^{n}$.\n",
    "\n",
    "Because the unit sphere $\\mathbb{S}^{n-1}\\subseteq \\mathbb{R}^{n}$ is a _compact set_ (a set that is closed and bounded in the mathematical sense), and the function $f({\\bf x})=\\|A{\\bf x}\\|_2$ is continuous, there is a particular choice ${\\bf x}_0\\in \\mathbb{S}^{n-1}$ that produces the supremum, i.e.\n",
    "    \n",
    "    \n",
    "$$\\|A\\|_2=\\|A{\\bf x}_0\\|_2.$$\n",
    "\n",
    "Because any candidate unit vector ${\\bf x}=\\sum_{i=1}^r c_i{\\bf v} = V_r{\\bf c}$ must be a linear combination of the right singular vectors from the reduced SVD of $A$, we have\n",
    "    \n",
    "$$\\|A{\\bf x}\\|_2^2={\\bf x}^tA^tA{\\bf x}={\\bf c}^tV_r^tA^tAV_r{\\bf c} = {\\bf c}^tV_r^tV_r\\Sigma_r U_r^tU_r\\Sigma_r V_r^tV_r{\\bf c}={\\bf c}^t\\Sigma_r^2{\\bf c}=\\sum_{i=1}^rc_i^2\\sigma_i^2\\leq \\sum_{i=1}^rc_i^2\\sigma_1^2 = \\sigma_1^2\\sum_{i=1}^rc_i^2 = \\sigma_1^2.$$\n",
    "    \n",
    "    \n",
    "Consequently, by choosing ${\\bf x}_0={\\bf v}_1$ (the right singular vector associated with the largest singular value $\\sigma_1$), we obtain the maximum value defining the matrix operator norm $\\|A\\|_2=\\sqrt{\\|A{\\bf x}_0\\|_2^2} = \\sigma_1$.\n",
    "\n",
    "Hence, the matrix norms of $A$ and $A^+$ are\n",
    "    \n",
    "$$\\|A\\|_2=\\sigma_1$$\n",
    "\n",
    "$$\\|A^+\\|_2=\\sigma_r^{-1}$$\n",
    "\n",
    "and the __condition number__ of $A$ can be expressed in terms of these norms as\n",
    "    \n",
    "$$\\kappa(A)={\\|A\\|_2}{\\|A^+\\|_2}=\\frac{\\sigma_1}{\\sigma_r}.$$\n",
    "\n",
    "The Julia-script \"__Condition_number.jl__\" demonstrates some unfavourable consequences for the solution of a system $A{\\bf x}={\\bf b}$ when\n",
    "$A$ has a _large condition number_. Such sysems are called _poorly conditioned_.\n",
    "\n",
    "\n",
    "</font> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank $k$ regularization  \n",
    "\n",
    "'\n",
    "\n",
    "<font size=\"4\">\n",
    "\n",
    "__Definition (Truncated pseudo-inverse)__   \n",
    "By omitting the terms in $\\sum_{i=1}^{n}\\sigma_i{\\bf u}_i{\\bf v}_i^t$ associated with some of the smaller singular values, we obtain a _truncated pseudoinverse_. By keeping the $k$ first terms the associated truncated pseudo-inverse of rank $k$ is\n",
    "$$A_k^+=V_k\\Sigma_k^{-1}U_k^t=\\sum_{i=1}^{k}\\sigma_i^{-1}{\\bf v}_i{\\bf u}_i^t.$$\n",
    "\n",
    "    \n",
    "__Definition (rank $k$ regularized solution)__\n",
    "\n",
    "$${\\bf x}=A_k^+{\\bf b}$$ \n",
    "    \n",
    "is called the rank $k$ _regularized_ solution of $A{\\bf x}={\\bf b}$.\n",
    "\n",
    "'   \n",
    "    \n",
    "The rank $k$ _regularized_ solution of $A{\\bf x}={\\bf b}$ (a.k.a. _reduced rank regression_) is closely related to the so-called _principal component regression (PCR)_ based on the first $k$ principal components of the coefficient matrix $A$ (were we assume that all the columns are centred to have mean values equal to $0$).\n",
    "\n",
    "</font>     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Principal component regression (PCR)](https://en.wikipedia.org/wiki/Principal_component_regression)\n",
    "\n",
    "'\n",
    "\n",
    "<font size=\"4\">\n",
    "Recall that for principal component analysis (PCA) we assume that the data matrix $X \\in \\mathbb{R}^{m\\times n}$ has rank $r$, and that each $X$-column is arranged according to a common ordering of observations of corresponding real-valued random vectors with mean zero. \n",
    "    \n",
    "The data matrix is always assumed to be centered, i.e. the mean of each $X$-column is equal to $0$ (zero).\n",
    "    \n",
    "From the SVD of the data matrix $X = U_r\\Sigma_r V_r^t$ we have: \n",
    "    \n",
    "* The right singular vectors ${\\bf v}_i$ ($i = 1,\\cdots,r$) from the columns of $V_r$ define _the principal components directions_ (also known as _the loadings_) of $X$.\n",
    "\n",
    "   \n",
    "    \n",
    "* The left singular vectors ${\\bf u}_i$ ($i = 1,\\cdots,r$) from the columns of $U_r$ define _the normalized principal components_, and the associated vectors ${\\bf t}_i=\\sigma_i{\\bf u}_i = X{\\bf v}_i$ are called the _principal component scores_ of $X$.  \n",
    "\n",
    "\n",
    "The truncated rank $k$ pseudo-inverse of $X$ is $X_k^+ = V_k\\Sigma_k^{-1}U_k^t = \\sum_{i=1}^{k}\\sigma_i^{-1}{\\bf v}_i{\\bf u}_i^t$. \n",
    "\n",
    "------------------------    \n",
    "    \n",
    "For a data matrix $X \\in \\mathbb{R}^{m\\times n}$ and response vector ${\\bf y}=\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2\\\\\n",
    " \\vdots      \\\\\n",
    "y_m\n",
    "\\end{bmatrix} \\in \\mathbb{R}^m$ the corresponding $k$-component __principal component regression (PCR) coefficients__ are defined as the rank $k$ solution \n",
    "\n",
    "$$\\hat{\\beta}_k = X_k^+{\\bf y} = X_k^+{\\bf y}_0.$$\n",
    "    \n",
    "of the system $X\\beta = {\\bf y}_0$, where ${\\bf y}_0 = {\\bf y}-\\bar{y}$, ($\\bar{y}=\\frac{1}{m}\\sum_{i=1}^m y_i$) is the mean centered version of ${\\bf y}$.\n",
    "    \n",
    "To predict the response value $\\hat{y}$ for a new datapoint (sample) ${\\bf x}^t\\in \\mathbb{R}^n$ based on the $k$-component __PCR-model__ we also include a constant term $\\beta_{0,k}$ to calculate\n",
    "    \n",
    "$$\\hat{y} = \\beta_{0,k} + {\\bf x}^t\\hat{\\beta}_k$$\n",
    "    \n",
    "for $\\beta_{0,k} = \\bar{y}-\\bar{\\bf x}^t\\hat{\\beta}_k$, where $\\bar{\\bf x}^t$ is the (row) vector of column means used for centering of the data matrix $X$ and $\\bar{y}=\\frac{1}{m}\\sum_{i=1}^m y_i$. Note that for the particular choice ${\\bf x} = \\bar{\\bf x}$ we obtain the prediction\n",
    "    \n",
    "$$\\hat{y} = \\beta_{0,k} + \\bar{\\bf x}^t\\hat{\\beta}_k = \\bar{y}-\\bar{\\bf x}^t\\hat{\\beta}_k + \\bar{\\bf x}^t\\hat{\\beta}_k = \\bar{y},$$    \n",
    "    \n",
    "i.e. from the mean of the observed $X$-data we predict the mean of the observed ${\\bf y}$-data.    \n",
    "    \n",
    "__`Exercise 4:`__       \n",
    "Let $M_k = [{\\bf u}_0\\ {\\bf u}_1 \\cdots {\\bf u}_k]\\in \\mathbb{R}^{m\\times (k+1)}$, where ${\\bf u}_0 = \\frac{1}{\\sqrt{m}}{\\bf 1} = \\begin{bmatrix}\n",
    "\\frac{1}{\\sqrt{m}} \\\\\n",
    "\\frac{1}{\\sqrt{m}}\\\\\n",
    " \\vdots      \\\\\n",
    "\\frac{1}{\\sqrt{m}}\n",
    "\\end{bmatrix}$ is the constant vector of norm 1 in $\\mathbb{R}^{m}$.\n",
    "\n",
    "* a) Explain why $M_k^tM_k=I_{k+1}$.\n",
    "\n",
    "    \n",
    "* b) The projection mapping onto the column space $Col(M_k)$ is given by $H_k = M_kM_k^t$. Verify that the projection $\\hat{\\bf y} = H_k{\\bf y}$ of ${\\bf y}$ onto $Col(M_k)$ is identical to the fitted values for the $X$-data of the $k$-component PCR model:\n",
    "    \n",
    "$$\\hat{\\bf y}={\\bf 1}\\beta_{0,k} + X_1\\hat{\\beta}_k,$$\n",
    "    \n",
    "where $X_1 = {\\bf 1}\\bar{\\bf x}^t + X$ is the uncentered version of the $m\\times n$ data matrix $X$.    \n",
    "\n",
    "</font> \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
