{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATH310 - Lecture_notes5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some particular bi-objective problems ([see §15.5.2 in VMLS](https://web.stanford.edu/~boyd/vmls/vmls.pdf#page=342))\n",
    "\n",
    "<font size=\"4\">\n",
    "'\n",
    " \n",
    "We focus on solving problems of the following type: \n",
    "    \n",
    "Find ${\\bf x}\\in \\mathbb{R}^{n}$ minimizing the bi-objective function\n",
    "    \n",
    "$$J({\\bf x}) = \\|A{\\bf x}-{\\bf b}\\|^2+ \\lambda\\|{\\bf x}-{\\bf x}^{des}\\|^2, $$        \n",
    "where the $m\\times n$ coefficient matrix $A$ is \"wide\" (meaning that $n>m$ i.e. we have more unknowns than equations in the system\n",
    "$A{\\bf x}={\\bf b}$) and the magnitude of $\\lambda > 0$ indicate the strength in our desire for the solution ${\\bf x}$ to be close to some (desired) ${\\bf x}^{des}\\in \\mathbb{R}^{n}$.  \n",
    "    \n",
    "With $A_1=A$, ${\\bf b}_1 = {\\bf b}$, $A_2 = I_n$, ${\\bf b}_2 = {\\bf x}^{des}$, $\\lambda_1 = 1$ and $\\lambda_2=\\lambda$ the above bi-objective function can be expressed as\n",
    "    \n",
    "$$J({\\bf x}) = \\lambda_1\\|A_1{\\bf x}-{\\bf b}_1\\|^2 + \\lambda_2\\|A_2{\\bf x}-{\\bf b}_2\\|^2,$$    \n",
    "    \n",
    "i.e. a _weigthed sum objective_ of a bi-objective least squares problem, see [§15.1 in VMLS](https://web.stanford.edu/~boyd/vmls/vmls.pdf#page=319).\n",
    "    \n",
    "</font>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An OLS-formulation of the above problem-type\n",
    "<font size=\"4\">\n",
    "'\n",
    " \n",
    "Note that the above objective function $J({\\bf x})$ corresponds to the ordinary least squares (OLS) formulation for solving the system\n",
    " \n",
    "    \n",
    "$$\\begin{bmatrix}\n",
    " A \\\\\n",
    "\\sqrt{\\lambda} I_n\n",
    "\\end{bmatrix}{\\bf x}= \\begin{bmatrix}\n",
    " {\\bf b} \\\\\n",
    "\\sqrt{\\lambda}{\\bf x}^{des}\n",
    "\\end{bmatrix}$$    \n",
    "       \n",
    "with the corresponding normal equations \n",
    "    \n",
    "$$(A^tA+\\lambda I_n){\\bf x} = A^t{\\bf b}+\\lambda{\\bf x}^{des}.$$    \n",
    "\n",
    "The least squares solution of this system is\n",
    "\n",
    "$$\\hat{\\bf x} = (A^tA+\\lambda I_n)^{-1}(A^t{\\bf b} + \\lambda{\\bf x}^{des})$$\n",
    "$$ = (A^tA+\\lambda I_n)^{-1}(A^t{\\bf b} + (\\lambda I_n + A^t A){\\bf x}^{des}-(A^tA){\\bf x}^{des})$$ \n",
    "$$ = (A^tA+\\lambda I_n)^{-1}A^t({\\bf b}-A{\\bf x}^{des})+{\\bf x}^{des}.$$    \n",
    "    \n",
    "Note that the inverted matrix $(A^tA+\\lambda I_n)^{-1}\\in \\mathbb{R}^{n\\times n}$.    \n",
    "    \n",
    "</font>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The \"kernel trick\" for faster solution of the above problem type\n",
    "<font size=\"4\">\n",
    "'\n",
    " \n",
    "Note that\n",
    "    \n",
    "$$(A^tA+\\lambda I_n)A^t = A^t(AA^t+\\lambda I_m),$$    \n",
    "    \n",
    "where both $(A^tA+\\lambda I_n)$ and $(AA^t+\\lambda I_m)$ are invertible matrices for $\\lambda > 0$.    \n",
    "    \n",
    "Multiplication of the above equation from the left by $(A^tA+\\lambda I_n)^{-1}$ and from the right by $(AA^t+\\lambda I_m)^{-1}$ yields the identity\n",
    "    \n",
    "$$A^t(AA^t+\\lambda I_m)^{-1} = (A^tA+\\lambda I_n)^{-1}A^t.$$ \n",
    "    \n",
    "Therefore the OLS solution of $\\begin{bmatrix}\n",
    " A \\\\\n",
    "\\sqrt{\\lambda} I_n\n",
    "\\end{bmatrix}{\\bf x}= \\begin{bmatrix}\n",
    " {\\bf b} \\\\\n",
    "\\sqrt{\\lambda}{\\bf x}^{des}\n",
    "\\end{bmatrix}$\n",
    "can also be expressed as\n",
    "\n",
    "\n",
    "$$\\hat{\\bf x} = A^t(AA^t+\\lambda I_m)^{-1}({\\bf b}-A{\\bf x}^{des})+{\\bf x}^{des}.$$\n",
    "    \n",
    "Note that here the inverted matrix $(AA^t+\\lambda I_m)^{-1}\\in \\mathbb{R}^{m\\times m}$, which is a smaller problem for wide matrices ($n>m$).   \n",
    "    \n",
    "If $QR = \\bar{A}=\\begin{bmatrix}\n",
    "  A^t \\\\\n",
    "  \\sqrt{\\lambda} I_m \n",
    "  \\end{bmatrix}$\n",
    "is _the qr-decomposition_ of the stacked $(n+m)\\times m$ matrix $\\bar{A}$. Then\n",
    "    \n",
    "$$(AA^t+\\lambda I_m) = \\bar{A}^t\\bar{A} = R^tQ^tQR = R^tR,$$    \n",
    "    \n",
    "and the OLS solution becomes\n",
    "    \n",
    "$$\\hat{\\bf x} = A^t(R)^{-1}(R^t)^{-1}({\\bf b}-A{\\bf x}^{des})+{\\bf x}^{des}.$$        \n",
    "    \n",
    "</font>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Tikhonov regularization (Ridge regression) modelling](https://en.wikipedia.org/wiki/Tikhonov_regularization)\n",
    "<font size=\"4\">\n",
    "'\n",
    " \n",
    "Let's convert to \"statistics notation\" where $X$ denotes a mean centered data matrix of size $m\\times n$ where typically $n>m$ (we have more variables/unknowns than samples),  ${\\bf y}_0$ is the corresponding mean centered response and $\\lambda>0$. Then, if the \"desired\" solution of the above problem type is set to ${\\bf 0}$, our minimization problem is about finding $\\beta\\in \\mathbb{R}^{n}$ minimizing the objective\n",
    "    \n",
    "$$J(\\beta) = \\|X\\beta-{\\bf y}_0\\|^2+ \\lambda\\|\\beta\\|^2. $$    \n",
    "    \n",
    "The corresponding OLS-problem is\n",
    "    \n",
    "$$\\begin{bmatrix}\n",
    "X \\\\\n",
    "\\sqrt{\\lambda} I_n\n",
    "\\end{bmatrix}\\beta= \\begin{bmatrix}\n",
    "{\\bf y}_0 \\\\\n",
    "{\\bf 0}\n",
    "\\end{bmatrix},$$    \n",
    "    \n",
    "where ${\\bf y}_0 = {\\bf y}-\\bar{y}$ ($\\bar{y}=\\frac{1}{m}\\sum_{i=1}^m y_i$) is the mean centered version of ${\\bf y}$.     \n",
    "    \n",
    "This type of OLS-problem is often called [Tikhonov regularization (TR) or Ridge regression (RR)](https://en.wikipedia.org/wiki/Tikhonov_regularization), see [§15.3.1 and §15.4 in VMLS](https://web.stanford.edu/~boyd/vmls/vmls.pdf#page=327).\n",
    "    \n",
    "According to the above derivations, the least squares solution of such problems is given by\n",
    "    \n",
    "$$\\beta_{\\lambda} = X^t(XX^t+\\lambda I_m)^{-1}{\\bf y}_0.$$    \n",
    "\n",
    "Analogously to PCR (see last weeks notes) we predict the response value $\\hat{y}$ for a new datapoint (sample) ${\\bf x}^t\\in \\mathbb{R}^n$ based on the $\\lambda$-regularized __RR-model__ by including a constant term $\\beta_{0,k}$ to calculate\n",
    "    \n",
    "$$\\hat{y} = \\beta_{0,\\lambda} + {\\bf x}^t\\beta_{\\lambda}.$$\n",
    "    \n",
    "Here $\\beta_{0,\\lambda} = \\bar{y}-\\bar{\\bf x}^t\\beta_{\\lambda}$ where $\\bar{\\bf x}^t$ is the (row) vector of column means used for centering of the data matrix $X$. \n",
    "    \n",
    "Note that for the particular choice ${\\bf x} = \\bar{\\bf x}$ we obtain the prediction\n",
    "    \n",
    "$$\\hat{y} = \\beta_{0,\\lambda} + \\bar{\\bf x}^t{\\beta}_{\\lambda} = \\bar{y}-\\bar{\\bf x}^t{\\beta}_{\\lambda} + \\bar{\\bf x}^t{\\beta}_{\\lambda} = \\bar{y},$$    \n",
    "    \n",
    "i.e. from the mean of the observed $X$-data we predict the mean of the observed ${\\bf y}$-data, just as we did for the PCR-models.\n",
    "    \n",
    "</font>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation and -selection\n",
    "\n",
    "<font size=\"4\">\n",
    "'\n",
    " \n",
    "__Question:__ How do we select the number of principal components ($k$) in PCR and the regularization parameter value ($\\lambda$) i RR to obtain models with good predictions?\n",
    "    \n",
    "__Answer:__ We can do [10-fold cross validation or leave-one-out cross validation (recall §13.2 in VMLS)](https://web.stanford.edu/~boyd/vmls/vmls.pdf#page=270) for the various candidate models, compare the RMS-values for the predictions to choose a model with seemingly low prediction error...\n",
    "    \n",
    "</font>  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
