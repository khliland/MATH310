{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATH310 - Lecture_notes3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">  \n",
    "\n",
    "__[The Singular value decomposition (SVD)](https://en.wikipedia.org/wiki/Singular_value_decomposition) and [Principal component analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis)__     \n",
    "    \n",
    "* Optimal matrix approximation \n",
    "    \n",
    "    \n",
    "* Maximum variance subspace identification\n",
    "        \n",
    "\n",
    "Recommended supplementary videos:\n",
    "\n",
    "* [Gilbert Strang about the SVD](https://ocw.mit.edu/courses/mathematics/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/video-lectures/lecture-6-singular-value-decomposition-svd/)    \n",
    "\n",
    "     \n",
    "* [Gilbert Strang about the Eckart-Young-Mirsky low rank approximation theorem](https://ocw.mit.edu/courses/mathematics/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/video-lectures/lecture-7-eckart-young-the-closest-rank-k-matrix-to-a/)    \n",
    "\n",
    "Recommended supplementary reading:\n",
    "\n",
    "* Chapter I.8 and I.9 in [_Linear Algebra and Learning from Data_](http://math.mit.edu/~gs/learningfromdata/)   \n",
    "    \n",
    "    \n",
    "* Chapter 6 in [_Matrix Methods in Data Mining and Pattern Recognition_](https://epubs.siam.org/doi/book/10.1137/1.9780898718867)\n",
    "\n",
    "    \n",
    "* Chapter 4.5 in [Mathematics for Machine Learning (pdf)](https://mml-book.github.io/book/mml-book.pdf)\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Singular value decomposition\n",
    "\n",
    "<font size=\"4\">\n",
    "\n",
    "Although the QR decomposition is highly useful for solving least squares problems,\n",
    "and has excellent numerical properties, it has a drawback in only providing an orthonormal basis for the column space of a matrix.\n",
    "\n",
    "\n",
    "* The __singular value decomposition (SVD)__ is a more sophisticated matrix factorization method. It simultaneously deals with both the column- and row spaces of a matrix by providing orthonomal bases for both in a symmetric fashion. Hence, it supplies more information about a matrix than any other matrix factorization method. \n",
    "\n",
    "    \n",
    "* The SVD also sorts the information content of a matrix so that its “dominant parts” becomes easily accessible. This is what makes the SVD so useful in both machine learning and may other areas of applied mathematics.\n",
    "\n",
    "    \n",
    "The following presentation of the SVD and PCA with illustrations are much influenced by the presentation in Lars Eldéns book [Matrix Methods in Data Mining and Pattern Recognition](https://epubs.siam.org/doi/book/10.1137/1.9780898718867).  \n",
    "  \n",
    "</font>   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theorem ([SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition))\n",
    "\n",
    "<font size=\"4\">\n",
    "    \n",
    "Any $m\\times n$ matrix ${\\bf A}$, with $m \\geq n$, can be factorized as\n",
    "\n",
    "$$ A = U\\Sigma V^t$$\n",
    "\n",
    "    \n",
    "where $U \\in \\mathbb{R}^{m\\times m}$ and $V \\in \\mathbb{R}^{n\\times n}$ are orthogonal, and $\\Sigma\\in \\mathbb{R}^{m\\times n}$ is diagonal with diagonal elements $\\sigma_1\\geq \\sigma_2\\geq ...\\geq \\sigma_n\\geq 0 \\ \\blacksquare$\n",
    "    \n",
    "\n",
    "Note that the assumption $m \\geq n$ in the SVD-theorem is not really a restriction since for matrices where $n>m$ we can transpose and apply the theorem to $A^t$ by just switching the notation for $U$ and $V$.\n",
    "    \n",
    "The columns of $U$ and $V$ are called the left- and right singular vectors, respectively, and the diagonal elements\n",
    "$\\sigma_i$ of $\\Sigma$ are called the _singular values_.\n",
    "\n",
    "Note that the SVD is much more than a fancy theoretical result. There are highly efficient and numerically accurate algorithms for [calculating the SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition#Calculating_the_SVD) making it highly powerful for real ML-problems.\n",
    "      \n",
    "  \n",
    "</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "\n",
    "Pictorially we can think of the SVD as follows\n",
    "  \n",
    " \n",
    "![Loss](SVD_full.png)\n",
    "\n",
    "and by ignoring the last $m-n$ columns of $U$ that are not contained in the column space of the matrix $A$ we obtain the corresponding \"thin\" version of the SVD:\n",
    "    \n",
    "![Loss](SVD_thin2.png)    \n",
    "\n",
    "    \n",
    "If we consider the associated matrix equations of the thin SVD including only $n$ columns in $U$, i.e.\n",
    "\n",
    "$$AV = U\\Sigma,\\ \\ \\ A^tU = V\\Sigma$$\n",
    "    \n",
    "column by column of $V=[{\\bf v}_1\\ {\\bf v}_2\\ \\cdots\\ {\\bf v}_n]$ and $U=[{\\bf u}_1\\ {\\bf u}_2\\ \\cdots\\ {\\bf u}_n]$, we get the equivalent equations    \n",
    "    \n",
    "$$A{\\bf v}_i =\\sigma_i{\\bf u}_i,\\ \\ \\ A^t{\\bf u}_i = \\sigma_i{\\bf v}_i,\\ \\ \\ i = 1,...,n.$$\n",
    "    \n",
    "</font>     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "    \n",
    "The SVD can also conveinently be written as an _outer product expansion_ of the matrix\n",
    "\n",
    "$$A = \\sum_{i = 1}^{n}\\sigma_i{\\bf u}_i{\\bf v}_i^t$$\n",
    "\n",
    "derived by starting from the detailed notation for the thin SVD\n",
    "\n",
    "$$A = U\\Sigma V^t = [{\\bf u}_1\\ {\\bf u}_2\\ \\cdots\\ {\\bf u}_n] \\begin{bmatrix}\n",
    " \\sigma_1 & & & \\\\\n",
    " & \\sigma_2 & & \\\\\n",
    " & & \\ddots & \\\\\n",
    " & & & \\sigma_n\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    " {\\bf v}_1^t \\\\\n",
    " {\\bf v}_2^t \\\\\n",
    " \\vdots      \\\\\n",
    " {\\bf v}_n^t\n",
    "\\end{bmatrix}$$ \n",
    " $$   = [\\sigma_1{\\bf u}_1\\ \\sigma_2{\\bf u}_2\\ \\cdots\\ \\sigma_n{\\bf u}_n]\\begin{bmatrix}\n",
    " {\\bf v}_1^t \\\\\n",
    " {\\bf v}_2^t \\\\\n",
    " \\vdots      \\\\\n",
    " {\\bf v}_n^t\n",
    "\\end{bmatrix} = \\sum_{i = 1}^{n}\\sigma_i{\\bf u}_i{\\bf v}_i^t.$$\n",
    "    \n",
    "If $A$ has not full rank, i.e. $rank(A)=r<n$ we must have the signgular values $\\sigma_1\\geq \\cdots \\geq\\sigma_r >\\sigma_{r+1}=\\sigma_{r+2}=\\cdots=\\sigma_n=0$, and the outer product expansion obviously requires only inclusion of the first $r$ terms:\n",
    "    \n",
    "$$A = \\sum_{i = 1}^{r}\\sigma_i{\\bf u}_i{\\bf v}_i^t. $$\n",
    "    \n",
    "</font>     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamental Subspaces\n",
    "\n",
    "<font size=\"4\">\n",
    "\n",
    "The SVD provides orthogonal bases for the four fundamental subspaces of a matrix $A$.\n",
    "    \n",
    "The range $\\mathcal{R}(A)$ of the matrix $A$, when considered as a linear transformation, is the linear subspace\n",
    "    \n",
    "$$\\mathcal{R}(A)=\\{{\\bf y}\\vert {\\bf y}=A{\\bf x}, \\textrm{ for some }{\\bf x}\\in \\mathbb{R}^n\\} = Col(A),$$\n",
    "    \n",
    "where $Col(A)$ denotes the column space of $A$ spanned by all possible linear combinations of the $A$-columns. \n",
    "\n",
    "If $rank(A)=r$, $Col(A)$ is spanned by the $r$ first left singular vectors ${\\bf u}_1,\\cdots, {\\bf u}_r$ corresponding to the  $r$ positive singular values $\\sigma_1\\geq\\sigma_2\\geq\\cdots\\geq\\sigma_r>\\sigma_{r+1}=0$ of $A$.\n",
    "    \n",
    "The null-space $\\mathcal{N}(A)$ of the matrix $A$ is the linear subspace    \n",
    "    \n",
    "$$\\mathcal{N}(A)=\\{{\\bf x}\\vert A{\\bf x}={\\bf 0}\\}.$$    \n",
    "    \n",
    "Since matrix-vector product \n",
    "    \n",
    "$$A{\\bf x}= (\\sum_{i=1}^{r}\\sigma_i{\\bf u}_i{\\bf v}_i^t){\\bf x} = \\sum_{i=1}^{r}\\sigma_i{\\bf u}_i({\\bf v}_i^t{\\bf x}),$$ \n",
    "    \n",
    "it is clear that only the vectors of the form\n",
    "    \n",
    "$${\\bf x}=\\sum_{j = r+1}^{n}c_j{\\bf v}_j,$$ \n",
    "    \n",
    "i.e. expressed as linear combinations of the right singular vectors corresponding to the singular values $\\sigma_{r+1}=\\sigma_{r+2}=\\cdots\\sigma_{n}=0$, will satisfy the requirements for membership in the null-space $\\mathcal{N}(A)$ due to the orthogonality of the right singular vectors ${\\bf v}_1,\\cdots,{\\bf v}_n$.\n",
    "    \n",
    "From the corresponding consideration of $A^t$ we can conclude the following theorem:    \n",
    "\n",
    "</font>     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theorem (The four fundamental subspaces of a matrix)\n",
    "'\n",
    "\n",
    "<font size=\"4\">\n",
    "\n",
    " 1.  The left singular vectors ${\\bf u}_1, {\\bf u}_2, \\cdots, {\\bf u}_r$ represent an orthonormal basis for $\\mathcal{R}(A)$ and \n",
    "    \n",
    "$$rank(A)=\\dim(\\mathcal{R}(A))=r.$$\n",
    " \n",
    "    \n",
    " 2.  The right singular vectors ${\\bf v}_{r+1}, {\\bf v}_{r+2}, \\cdots, {\\bf v}_n$ corresponding to the $0$-singular values of $A$ is an orthonormal basis for $\\mathcal{N}(A)$ and \n",
    "    \n",
    "$$\\dim(\\mathcal{N}(A))=n-r.$$ \n",
    "    \n",
    "    \n",
    " 3. The right singular vectors ${\\bf v}_{1}, {\\bf v}_{2}, \\cdots, {\\bf v}_r$ represent an orthonormal basis for $\\mathcal{R}(A^t)$ and \n",
    "    \n",
    " $$rank(A^t)=\\dim(\\mathcal{R}(A))=r.$$\n",
    "    \n",
    "    \n",
    " 4. The left singular vectors ${\\bf u}_{r+1}, {\\bf u}_{r+2}, \\cdots, {\\bf u}_m$ represent an orthonormal basis for $\\mathcal{N}(A^t)$ and \n",
    "    \n",
    " $$\\dim(\\mathcal{N}(A^t))=m-r.$$   \n",
    "\n",
    "\n",
    "    \n",
    "</font>     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Approximation - the truncated SVD\n",
    "'\n",
    "<font size=\"4\">\n",
    "\n",
    "Assume that $A$ is the result of a low rank matrix plus noise, i.e. \n",
    "\n",
    "'   \n",
    "$$A = A_0 + N,$$ \n",
    "    \n",
    "where the matrix $N$ representing the noise is small compared to $A_0$. In this situation the singular values of $A$ will typically show a pattern as illustrated in the following figure: \n",
    "    \n",
    "![Loss](Singular_values_and_noise.png)    \n",
    "\n",
    "In this situation the noisy part of $A$ is typically significantly smaller in magnitude. \n",
    "\n",
    "The number of large singular values is often referred to as _the numerical rank_ of the matrix. \n",
    "    \n",
    "If we know the correct rank of $A_0$, or are confident in estimating it from inspection of the singular values of $A$, we can “remove the noisy part” by \"truncation\" (by setting the smalerl singular values to $0$) and approximate the desired $A_0$ by a matrix $A_k$ of the correct rank. \n",
    "    \n",
    "According to this idea, the approximation of $A_0$ is obtained by zero-ing out the significantly smaller singular values $\\sigma_{k+1}, \\sigma_{k+1},\\cdots,\\sigma_r$ in the expansion \n",
    "    \n",
    "$$A = \\sum_{i = 1}^{r}\\sigma_i{\\bf u}_i{\\bf v}_i^t.$$ \n",
    "    \n",
    "The resulting truncaterd approximation $A_k$ is then given by\n",
    "    \n",
    "$$A =\\sum_{i = 1}^{r}\\sigma_i{\\bf u}_i{\\bf v}_i^t \\approx \\sum_{j = 1}^{k}\\sigma_j{\\bf u}_j{\\bf v}_j^t\\ \\stackrel{\\text{def}}{=}\\ A_k.$$  \n",
    "    \n",
    "The idea of SVD-truncation is powerful, not only for removing noise i the $X$-data, but also for compressing data and for stabilizing the solution of problems that are extremely ill-conditioned (situations where the [condition number](https://en.wikipedia.org/wiki/Condition_number#:~:text=If%20the%20condition%20number%20is%20very%20large%2C%20then%20the%20matrix,condition%20number%20equal%20to%20infinity) $\\sigma_1/\\sigma_r$ of $A$ is large). \n",
    "    \n",
    "It turns out that the truncated SVD is also the mathematically optimal solution in approximation problems, where one wants the best possible low rank approximation a given matrix $A$.    \n",
    "    \n",
    "</font>     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "\n",
    "The _$2$-norm_ of a $n\\times m$-matrix $A$ is denoted $\\|A\\|_2$ and is induced by the $2$-norm ([Eucidean norm](https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm)) maximization of the resulting vectors $A{\\bf x}$ restricted to $A$-multiplications with ${\\bf x}$-vectors from the [unit sphere](https://en.wikipedia.org/wiki/Unit_sphere) of $\\mathbb{R}^m$:\n",
    "    \n",
    "$$\\|A\\|_2\\stackrel{\\text{def}}{=}\\sup_{\\|{\\bf x}\\|_2=1}\\|A{\\bf x}\\|_2.$$ \n",
    "    \n",
    "The _Frobenius norm_ of a $n\\times m$-matrix $A$ is denoted $\\|A\\|_F$ and defined by\n",
    "    \n",
    "$$\\|A\\|_F \\stackrel{\\text{def}}{=} \\sqrt{\\sum_{i=1}^m\\sum_{j = 1}^m a_{ij}^2},$$    \n",
    "\n",
    "i.e. the square root of all the squared matrix entries ($a_{ij}$) added together.    \n",
    "    \n",
    "</font>  \n",
    "\n",
    "## The Eckart–Young–Mirsky theorem  (low-rank approximation of matrices) in two versions\n",
    "'\n",
    "\n",
    "<font size=\"4\">\n",
    "Assume that the matrix $A \\in \\mathbb{R}^{m\\times n}$ has rank $r > k$. The two rank $k$ matrix\n",
    "approximation problems\n",
    "    \n",
    "$$\\min_{rank(Z)=k}\\|A-Z\\|_2 \\textrm{  and } \\min_{rank(Z)=k}\\|A-Z\\|_F $$\n",
    "\n",
    "both has the same solution\n",
    "    \n",
    "$$Z=A_k \\stackrel{\\text{def}}{=} U_k\\Sigma_k V_k^t,$$    \n",
    "    \n",
    "where $U_k = [{\\bf u}_1\\ {\\bf u}_2\\ \\cdots\\ {\\bf u}_k]$, $V_k = [{\\bf v}_1\\ {\\bf v}_2\\ \\cdots\\ {\\bf v}_k]$, and $\\Sigma_k = diag(\\sigma_1,\\cdots,\\sigma_k)$. \n",
    "    \n",
    "The corresponding minimum values are \n",
    "    \n",
    "$$\\|A-A_k\\|_2=\\sigma_{k+1} \\textrm{  and } \\|A-A_k\\|_F=\\sqrt{\\sum_{i= k+1}^{p}\\sigma_i^2},$$ \n",
    "    \n",
    "respectively, where $p = \\min\\{m,n\\}$.   \n",
    "\n",
    "[__A proof of both versions of the Eckart–Young–Mirsky theorem can be found here__](https://en.wikipedia.org/wiki/Low-rank_approximation).    \n",
    "    \n",
    "A pictorial illustration of the optimal $k$ rank matrix approximation $A_k =  U_k\\Sigma_k V_k^t$ of $A$ is\n",
    "    \n",
    "![Loss](SVD_approx.png)   \n",
    "    \n",
    "</font>   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Principal Component Analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis)\n",
    "<font size=\"4\">\n",
    "'\n",
    "    \n",
    "* PCA is much used for both [unsupervised learning](https://en.wikipedia.org/wiki/Unsupervised_learning#:~:text=Unsupervised%20learning%20(UL)%20is%20a,tagged%20by%20a%20human%2C%20eg) problems, [exploratory data analysis](https://en.wikipedia.org/wiki/Exploratory_data_analysis) and for making [predictive models](https://en.wikipedia.org/wiki/Predictive_modeling). \n",
    "    \n",
    "    \n",
    "* It is highly useful for dimensionality reduction by projecting the data points onto a set of relatively few principal components to obtain a lower-dimensional approximation of the data that preserves as much of the original data variation as possible.\n",
    " \n",
    "    \n",
    "* The optimal approximation properties of the SVD are appropriate for explaining the fundamental aspects of principal component analysis (PCA). \n",
    "\n",
    "    \n",
    "In the following we assume that the data matrix $X \\in \\mathbb{R}^{m\\times n}$ has rank $r$, and that each $X$-column is arranged according to a common ordering of observations of corresponding real-valued random vectors with mean zero. The matrix is assumed to be centered, i.e. the mean of each $X$-column is equal to zero.\n",
    "\n",
    "The SVD of the data matrix $X = U_r\\Sigma_r V_r^t$, and the right singular vectors ${\\bf v}_i$ ($i = 1,\\cdots,r$) are called _the principal components directions_ (also known as _the loadings_) of $X$. \n",
    "    \n",
    "The _first principal component_ vector\n",
    "\n",
    "$${\\bf t}_1 = X{\\bf v}_1=\\sigma_1{\\bf u}_1$$\n",
    "\n",
    "has the largest possible sample variance among all the possible normalized linear combinations of the $X$-columns with:\n",
    "    \n",
    "$$Var({\\bf t}_1)= Var(X{\\bf v}_1)=\\frac{\\sigma_1^2}{m}.$$    \n",
    "\n",
    "In linear algebra terminology finding the vector of maximal variance is equivalent to maximizing the [Rayleigh quotient](https://en.wikipedia.org/wiki/Rayleigh_quotient)\n",
    "    \n",
    "$$\\sigma_1^2 = \\max_{{\\bf v}\\neq{\\bf 0}}\\frac{{\\bf v}^tX^tX{\\bf v}}{{\\bf v}^t{\\bf v}} \\textrm{ where the corresponding optimal solution vector } {\\bf v}_1=\\arg\\max_{{\\bf v}\\neq{\\bf 0}}\\frac{{\\bf v}^tX^tX{\\bf v}}{{\\bf v}^t{\\bf v}}.$$    \n",
    "    \n",
    "* The normalized variable ${\\bf u}_1$ is called the _normalized first principal component_ of\n",
    "$X$.\n",
    "    \n",
    "\n",
    "* The second principal component ${\\bf t}_2 = X{\\bf v}_2=\\sigma_2{\\bf u}_2$ is the vector of the largest possible sample variance that can be obtained from the\n",
    "deflated data matrix $X_{(1)} = X−\\sigma_1{\\bf u}_1{\\bf v}_1^t$, and so on. \n",
    "\n",
    "\n",
    "* Equivalently, any subsequent principal component is defined as the vector of maximal variance subject to the constraint\n",
    "that it must be orthogonal to the previous ones.    \n",
    "    \n",
    "The following figure illustrates PCA of a data matrix $X\\in \\mathbb{R}^{500 \\times 3}$ containing 500 data points generated artificially from a correlated normal distribution in $\\mathbb{R}^3$.\n",
    "\n",
    "The original data points and the principal components are illustrated in the upper plot. After a deflation of $X$ with respect to $\\sigma_1{\\bf u}_1{\\bf v}_1^t$, the corresponding data points of the deflated $X_{(1)} =  X − \\sigma_1{\\bf u}_1{\\bf v}_1^t$ are shown in the bottom plot:    \n",
    "    \n",
    "![Loss](PCA_plot.png)\n",
    "\n",
    "It is straightforward to show that the principal components are eigenvectors of the empirical covariance matrix \n",
    "    \n",
    "$$C =\\frac{1}{m}X^tX.$$ \n",
    "    \n",
    "Thus, the principal components can also be found from the eigendecomposition of the $n\\times n$-matrix $C$. \n",
    "    \n",
    "Since the scalar $\\frac{1}{m}$ does not affect the resulting the principal components, it can be omitted from the calculations without loss of information. \n",
    "    \n",
    "* The amount of (empirical) $X$-variance _explained_ by the first $k$ principal compoenets is given by\n",
    "    \n",
    "$$ \\frac{\\sum_{i=1}^{k}\\sigma_i^2}{m}.$$\n",
    "    \n",
    "*  The corresponding amount in percent (%) of $X$-variance _explained_ by the first $k$ principal compoenets is       \n",
    "\n",
    "$$100\\cdot\\frac{\\sum_{i=1}^{k}\\sigma_i^2}{\\sum_{i=1}^{p}\\sigma_i^2},$$\n",
    "    \n",
    "where $p=\\min\\{m,n\\}$.\n",
    "    \n",
    "</font>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
